{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"name\": \"website_crawler_with_diff_detection.ipynb\",\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"view-in-github\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Webサイトクローラー（差分検知機能付き）- Google Colab版\\n\",\n",
    "        \"\\n\",\n",
    "        \"このノートブックでは、指定したURLから始めて同一ドメイン内のすべてのページをクロールし、Markdown形式で出力します。さらに、前回のクロール結果との差分を検出し、レポートを生成します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"setup-section\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 1. 必要なライブラリのインストール\\n\",\n",
    "        \"\\n\",\n",
    "        \"最初に必要なライブラリをインストールします。PDF生成のためのwkhtmltopdfも導入します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"install-libraries\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"!apt-get update\\n\",\n",
    "        \"!apt-get install -y wkhtmltopdf\\n\",\n",
    "        \"!pip install requests html2text lxml markdown pdfkit discord-webhook\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"drive-mount\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 2. Google Driveのマウント\\n\",\n",
    "        \"\\n\",\n",
    "        \"クロール結果やキャッシュを永続的に保存するために、Google Driveをマウントします。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"mount-drive-code\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"from google.colab import drive\\n\",\n",
    "        \"drive.mount('/content/drive')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# クローラー用のディレクトリを作成\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"crawler_dir = '/content/drive/MyDrive/website_crawler'\\n\",\n",
    "        \"output_dir = os.path.join(crawler_dir, 'output')\\n\",\n",
    "        \"cache_dir = os.path.join(crawler_dir, 'cache')\\n\",\n",
    "        \"\\n\",\n",
    "        \"os.makedirs(crawler_dir, exist_ok=True)\\n\",\n",
    "        \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"クローラーディレクトリを作成しました: {crawler_dir}\\\")\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"crawler-code\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 3. Webサイトクローラーのコード\\n\",\n",
    "        \"\\n\",\n",
    "        \"以下にWebサイトクローラーのコードを記述します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"code-cell\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"#!/usr/bin/env python3\\n\",\n",
    "        \"# -*- coding: utf-8 -*-\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"Webサイトクローラー：同一ドメイン内のすべてのコンテンツをMarkdown形式で出力し、\\n\",\n",
    "        \"完了後にDiscordに通知を送信するプログラム\\n\",\n",
    "        \"前回からの差分検知機能付き\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import requests\\n\",\n",
    "        \"import html2text\\n\",\n",
    "        \"from urllib.parse import urlparse, urljoin\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import hashlib\\n\",\n",
    "        \"from collections import deque\\n\",\n",
    "        \"from typing import Set, Dict, List, Optional, Tuple, Any\\n\",\n",
    "        \"import markdown\\n\",\n",
    "        \"import pdfkit\\n\",\n",
    "        \"from discord_webhook import DiscordWebhook, DiscordEmbed\\n\",\n",
    "        \"import lxml.html\\n\",\n",
    "        \"import argparse\\n\",\n",
    "        \"import sqlite3\\n\",\n",
    "        \"from datetime import datetime\\n\",\n",
    "        \"import difflib\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class UrlFilter:\\n\",\n",
    "        \"    \\\"\\\"\\\"URLをフィルタリングして、同一ドメイン内のURLのみを許可するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, base_url: str):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLフィルタークラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            base_url (str): クロールする基本URL\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.base_domain = urlparse(base_url).netloc\\n\",\n",
    "        \"        self.base_url = base_url\\n\",\n",
    "        \"        self.static_extensions = {\\n\",\n",
    "        \"            '.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', \\n\",\n",
    "        \"            '.js', '.pdf', '.zip', '.tar', '.gz', '.mp3', \\n\",\n",
    "        \"            '.mp4', '.avi', '.mov', '.webm', '.webp', '.ico'\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def normalize_url(self, url: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLを正規化する（相対URLを絶対URLに変換、フラグメントの削除等）\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): 正規化する URL\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: 正規化された URL\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        # 相対URLを絶対URLに変換\\n\",\n",
    "        \"        normalized_url = urljoin(self.base_url, url)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # フラグメント (#) を削除\\n\",\n",
    "        \"        normalized_url = normalized_url.split('#')[0]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # トレーリングスラッシュを統一\\n\",\n",
    "        \"        if normalized_url.endswith('/'):\\n\",\n",
    "        \"            normalized_url = normalized_url[:-1]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        return normalized_url\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def should_crawl(self, url: str) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLがクロール対象かどうかを判定する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): 判定する URL\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            bool: クロール対象の場合は True、そうでない場合は False\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        # 空のURLはクロールしない\\n\",\n",
    "        \"        if not url:\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # URLを正規化\\n\",\n",
    "        \"        url = self.normalize_url(url)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # URLのドメインを取得\\n\",\n",
    "        \"        parsed_url = urlparse(url)\\n\",\n",
    "        \"        domain = parsed_url.netloc\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 同一ドメインでない場合はクロールしない\\n\",\n",
    "        \"        if domain != self.base_domain:\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 静的ファイルはクロールしない\\n\",\n",
    "        \"        path = parsed_url.path.lower()\\n\",\n",
    "        \"        if any(path.endswith(ext) for ext in self.static_extensions):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # メールアドレスリンクはクロールしない\\n\",\n",
    "        \"        if url.startswith('mailto:'):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 電話番号リンクはクロールしない\\n\",\n",
    "        \"        if url.startswith('tel:'):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        return True\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Fetcher:\\n\",\n",
    "        \"    \\\"\\\"\\\"指定されたURLからHTMLコンテンツを取得するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, delay: float = 1.0, max_retries: int = 3, timeout: int = 10):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Fetcherクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            delay (float): リクエスト間の遅延秒数（サーバー負荷軽減のため）\\n\",\n",
    "        \"            max_retries (int): 最大再試行回数\\n\",\n",
    "        \"            timeout (int): リクエストタイムアウト秒数\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.delay = delay\\n\",\n",
    "        \"        self.max_retries = max_retries\\n\",\n",
    "        \"        self.timeout = timeout\\n\",\n",
    "        \"        self.last_request_time = 0\\n\",\n",
    "        \"        self.headers = {\\n\",\n",
    "        \"            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\\n\",\n",
    "        \"            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\\n\",\n",
    "        \"            'Accept-Language': 'en-US,en;q=0.5',\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def fetch(self, url: str, etag: Optional[str] = None, last_modified: Optional[str] = None) -> Tuple[Optional[str], Dict[str, str]]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLからHTMLコンテンツを取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): コンテンツを取得するURL\\n\",\n",
    "        \"            etag (Optional[str]): 前回取得時のETag\\n\",\n",
    "        \"            last_modified (Optional[str]): 前回取得時のLast-Modified\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Tuple[Optional[str], Dict[str, str]]: (取得したHTMLコンテンツ, レスポンスヘッダー情報)\\n\",\n",
    "        \"                                                 取得失敗時はコンテンツはNone\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        # リクエスト間隔を確保する\\n\",\n",
    "        \"        elapsed = time.time() - self.last_request_time\\n\",\n",
    "        \"        if elapsed < self.delay:\\n\",\n",
    "        \"            time.sleep(self.delay - elapsed)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 条件付きリクエスト用ヘッダーを準備\\n\",\n",
    "        \"        headers = self.headers.copy()\\n\",\n",
    "        \"        if etag:\\n\",\n",
    "        \"            headers['If-None-Match'] = etag\\n\",\n",
    "        \"        if last_modified:\\n\",\n",
    "        \"            headers['If-Modified-Since'] = last_modified\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        retries = 0\\n\",\n",
    "        \"        while retries <= self.max_retries:\\n\",\n",
    "        \"            try:\\n\",\n",
    "        \"                self.last_request_time = time.time()\\n\",\n",
    "        \"                response = requests.get(url, headers=headers, timeout=self.timeout)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # 304 Not Modified の場合、コンテンツは変更されていない\\n\",\n",
    "        \"                if response.status_code == 304:\\n\",\n",
    "        \"                    logging.info(f\\\"Content not modified: {url}\\\")\\n\",\n",
    "        \"                    return None, {\\n\",\n",
    "        \"                        'etag': etag,\\n\",\n",
    "        \"                        'last_modified': last_modified,\\n\",\n",
    "        \"                        'status_code': 304\\n\",\n",
    "        \"                    }\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # ステータスコードが200以外の場合は失敗とみなす\\n\",\n",
    "        \"                if response.status_code != 200:\\n\",\n",
    "        \"                    logging.warning(f\\\"Failed to fetch {url}: status code {response.status_code}\\\")\\n\",\n",
    "        \"                    retries += 1\\n\",\n",
    "        \"                    time.sleep(self.delay * (2 ** retries))  # 指数バックオフ\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # content-typeがHTMLでない場合はスキップ\\n\",\n",
    "        \"                content_type = response.headers.get('Content-Type', '')\\n\",\n",
    "        \"                if 'text/html' not in content_type.lower():\\n\",\n",
    "        \"                    logging.info(f\\\"Skipping non-HTML content: {url}, Content-Type: {content_type}\\\")\\n\",\n",
    "        \"                    return None, {'status_code': response.status_code, 'content_type': content_type}\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # ヘッダー情報を取得\\n\",\n",
    "        \"                headers_info = {\\n\",\n",
    "        \"                    'etag': response.headers.get('ETag'),\\n\",\n",
    "        \"                    'last_modified': response.headers.get('Last-Modified'),\\n\",\n",
    "        \"                    'content_type': content_type,\\n\",\n",
    "        \"                    'status_code': response.status_code\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                return response.text, headers_info\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            except requests.RequestException as e:\\n\",\n",
    "        \"                logging.error(f\\\"Error fetching {url}: {e}\\\")\\n\",\n",
    "        \"                retries += 1\\n\",\n",
    "        \"                if retries <= self.max_retries:\\n\",\n",
    "        \"                    time.sleep(self.delay * (2 ** retries))  # 指数バックオフ\\n\",\n",
    "        \"                else:\\n\",\n",
    "        \"                    return None, {'status_code': 0, 'error': str(e)}\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return None, {'status_code': 0, 'error': 'Max retries exceeded'}\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Parser:\\n\",\n",
    "        \"    \\\"\\\"\\\"HTMLコンテンツを解析し、コンテンツとリンクを抽出するコンポーネント（BeautifulSoup非使用）\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, url_filter: UrlFilter):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Parserクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url_filter (UrlFilter): URLフィルターインスタンス\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.url_filter = url_filter\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def parse(self, html: str, url: str) -> Tuple[Dict, List[str]]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        HTMLからコンテンツとリンクを抽出する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            html (str): 解析するHTMLコンテンツ\\n\",\n",
    "        \"            url (str): HTMLのURL（リンクの絶対URL化に使用）\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Tuple[Dict, List[str]]: (抽出したコンテンツ, 抽出したリンクのリスト)\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            # lxmlを使用してHTMLを解析\\n\",\n",
    "        \"            doc = lxml.html.fromstring(html)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # タイトルを抽出\\n\",\n",
    "        \"            title_elem = doc.xpath('//title')\\n\",\n",
    "        \"            title = title_elem[0].text_content().strip() if title_elem else \\\"No Title\\\"\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # メインコンテンツを抽出 (lxmlのXPath機能を使用)\\n\",\n",
    "        \"            content_selectors = [\\n\",\n",
    "        \"                '//main', '//article', \\n\",\n",
    "        \"                '//div[@class=\\\"content\\\"]', '//div[@id=\\\"content\\\"]', \\n\",\n",
    "        \"                '//div[@class=\\\"post-content\\\"]'\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            content_elem = None\\n\",\n",
    "        \"            for selector in content_selectors:\\n\",\n",
    "        \"                elements = doc.xpath(selector)\\n\",\n",
    "        \"                if elements:\\n\",\n",
    "        \"                    content_elem = elements[0]\\n\",\n",
    "        \"                    break\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # メインコンテンツが見つからない場合はbody全体を使用\\n\",\n",
    "        \"            if not content_elem:\\n\",\n",
    "        \"                body_elem = doc.xpath('//body')\\n\",\n",
    "        \"                content_elem = body_elem[0] if body_elem else doc\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # HTMLコンテンツを取得（lxml.html.tostring を使用）\\n\",\n",
    "        \"            html_content = lxml.html.tostring(content_elem, encoding='unicode')\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # リンクを抽出\\n\",\n",
    "        \"            links = []\\n\",\n",
    "        \"            for a_tag in doc.xpath('//a[@href]'):\\n\",\n",
    "        \"                href = a_tag.get('href')\\n\",\n",
    "        \"                if self.url_filter.should_crawl(href):\\n\",\n",
    "        \"                    normalized_url = self.url_filter.normalize_url(href)\\n\",\n",
    "        \"                    links.append(normalized_url)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # ページ情報の辞書を作成\\n\",\n",
    "        \"            page_data = {\\n\",\n",
    "        \"                'url': url,\\n\",\n",
    "        \"                'title': title,\\n\",\n",
    "        \"                'html_content': html_content,\\n\",\n",
    "        \"            }\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            return page_data, links\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error parsing HTML from {url}: {e}\\\")\\n\",\n",
    "        \"            # エラー時は空のデータと空のリンクリストを返す\\n\",\n",
    "        \"            return {'url': url, 'title': 'Error', 'html_content': ''}, []\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class MarkdownConverter:\\n\",\n",
    "        \"    \\\"\\\"\\\"HTMLコンテンツをMarkdown形式に変換するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        MarkdownConverterクラスの初期化\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.converter = html2text.HTML2Text()\\n\",\n",
    "        \"        self.converter.ignore_links = False\\n\",\n",
    "        \"        self.converter.ignore_images = False\\n\",\n",
    "        \"        self.converter.ignore_tables = False\\n\",\n",
    "        \"        self.converter.body_width = 0  # 行の折り返しを無効化\\n\",\n",
    "        \"        self.converter.unicode_snob = True  # Unicode文字を維持\\n\",\n",
    "        \"        self.converter.single_line_break = True  # 単一の改行を維持\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def convert(self, page_data: Dict) -> Dict:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        HTMLをMarkdownに変換する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            page_data (Dict): 変換するページデータ\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Dict: Markdownに変換されたページデータ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        title = page_data['title']\\n\",\n",
    "        \"        html_content = page_data['html_content']\\n\",\n",
    "        \"        url = page_data['url']\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # HTMLをMarkdownに変換\\n\",\n",
    "        \"        markdown_content = self.converter.handle(html_content)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Markdownタイトルを作成\\n\",\n",
    "        \"        markdown_title = f\\\"# {title}\\\\n\\\\n\\\"\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # URL情報を追加\\n\",\n",
    "        \"        url_info = f\\\"*Source: {url}*\\\\n\\\\n\\\"\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 最終的なMarkdownコンテンツを組み立て\\n\",\n",
    "        \"        full_markdown = markdown_title + url_info + markdown_content\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 結果を返す\\n\",\n",
    "        \"        result = page_data.copy()\\n\",\n",
    "        \"        result['markdown_content'] = full_markdown\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return result\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class CrawlCache:\\n\",\n",
    "        \"    \\\"\\\"\\\"クロール結果を永続的に保存し、差分検知に使用するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, domain: str, cache_dir: str = \\\"cache\\\"):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        CrawlCacheクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            domain (str): キャッシュを保存するドメイン名\\n\",\n",
    "        \"            cache_dir (str): キャッシュディレクトリ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.domain = domain\\n\",\n",
    "        \"        self.cache_dir = cache_dir\\n\",\n",
    "        \"        os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        self.db_path = os.path.join(cache_dir, f\\\"{domain}.db\\\")\\n\",\n",
    "        \"        self._initialize_db()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def _initialize_db(self):\\n\",\n",
    "        \"        \\\"\\\"\\\"データベースを初期化する\\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # pages テーブルを作成\\n\",\n",
    "        \"        cursor.execute('''\\n\",\n",
    "        \"        CREATE TABLE IF NOT EXISTS pages (\\n\",\n",
    "        \"            url TEXT PRIMARY KEY,\\n\",\n",
    "        \"            title TEXT,\\n\",\n",
    "        \"            content_hash TEXT,\\n\",\n",
    "        \"            etag TEXT,\\n\",\n",
    "        \"            last_modified TEXT,\\n\",\n",
    "        \"            last_crawled TEXT,\\n\",\n",
    "        \"            markdown_content TEXT\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        ''')\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # crawl_history テーブルを作成\\n\",\n",
    "        \"        cursor.execute('''\\n\",\n",
    "        \"        CREATE TABLE IF NOT EXISTS crawl_history (\\n\",\n",
    "        \"            id INTEGER PRIMARY KEY AUTOINCREMENT,\\n\",\n",
    "        \"            crawl_date TEXT,\\n\",\n",
    "        \"            page_count INTEGER,\\n\",\n",
    "        \"            new_count INTEGER,\\n\",\n",
    "        \"            updated_count INTEGER,\\n\",\n",
    "        \"            deleted_count INTEGER\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        ''')\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.commit()\\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_page(self, url: str) -> Optional[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLに対応するキャッシュされたページ情報を取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): 取得するページのURL\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Optional[Dict]: キャッシュされたページ情報、存在しない場合はNone\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        conn.row_factory = sqlite3.Row\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT * FROM pages WHERE url = ?', (url,))\\n\",\n",
    "        \"        row = cursor.fetchone()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if row:\\n\",\n",
    "        \"            return dict(row)\\n\",\n",
    "        \"        return None\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def add_or_update_page(self, page_data: Dict) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        ページ情報をキャッシュに追加または更新する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            page_data (Dict): 追加/更新するページデータ\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            bool: 更新された場合はTrue、新規追加の場合はFalse\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        url = page_data['url']\\n\",\n",
    "        \"        title = page_data['title']\\n\",\n",
    "        \"        markdown_content = page_data.get('markdown_content', '')\\n\",\n",
    "        \"        content_hash = self._compute_hash(markdown_content)\\n\",\n",
    "        \"        etag = page_data.get('etag')\\n\",\n",
    "        \"        last_modified = page_data.get('last_modified')\\n\",\n",
    "        \"        last_crawled = datetime.now().isoformat()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 既存のページかチェック\\n\",\n",
    "        \"        cursor.execute('SELECT content_hash FROM pages WHERE url = ?', (url,))\\n\",\n",
    "        \"        row = cursor.fetchone()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        is_update = row is not None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if is_update:\\n\",\n",
    "        \"            # 更新\\n\",\n",
    "        \"            cursor.execute('''\\n\",\n",
    "        \"            UPDATE pages \\n\",\n",
    "        \"            SET title = ?, content_hash = ?, etag = ?, last_modified = ?, \\n\",\n",
    "        \"                last_crawled = ?, markdown_content = ?\\n\",\n",
    "        \"            WHERE url = ?\\n\",\n",
    "        \"            ''', (title, content_hash, etag, last_modified, last_crawled, markdown_content, url))\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            # 新規追加\\n\",\n",
    "        \"            cursor.execute('''\\n\",\n",
    "        \"            INSERT INTO pages \\n\",\n",
    "        \"            (url, title, content_hash, etag, last_modified, last_crawled, markdown_content)\\n\",\n",
    "        \"            VALUES (?, ?, ?, ?, ?, ?, ?)\\n\",\n",
    "        \"            ''', (url, title, content_hash, etag, last_modified, last_crawled, markdown_content))\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.commit()\\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return is_update\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_all_urls(self) -> Set[str]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        キャッシュに保存されているすべてのURLを取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Set[str]: キャッシュされているすべてのURL\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT url FROM pages')\\n\",\n",
    "        \"        urls = {row[0] for row in cursor.fetchall()}\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return urls\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def delete_urls(self, urls: List[str]) -> int:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        指定されたURLをキャッシュから削除する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            urls (List[str]): 削除するURLのリスト\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            int: 削除されたURLの数\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        if not urls:\\n\",\n",
    "        \"            return 0\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        placeholders = ', '.join(['?'] * len(urls))\\n\",\n",
    "        \"        cursor.execute(f'DELETE FROM pages WHERE url IN ({placeholders})', urls)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        deleted_count = cursor.rowcount\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.commit()\\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return deleted_count\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def save_crawl_history(self, page_count: int, new_count: int, updated_count: int, deleted_count: int) -> int:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        クロール履歴を保存する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            page_count (int): クロールしたページの総数\\n\",\n",
    "        \"            new_count (int): 新規追加されたページ数\\n\",\n",
    "        \"            updated_count (int): 更新されたページ数\\n\",\n",
    "        \"            deleted_count (int): 削除されたページ数\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            int: 履歴のID\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        crawl_date = datetime.now().isoformat()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('''\\n\",\n",
    "        \"        INSERT INTO crawl_history \\n\",\n",
    "        \"        (crawl_date, page_count, new_count, updated_count, deleted_count)\\n\",\n",
    "        \"        VALUES (?, ?, ?, ?, ?)\\n\",\n",
    "        \"        ''', (crawl_date, page_count, new_count, updated_count, deleted_count))\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        history_id = cursor.lastrowid\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.commit()\\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return history_id\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_latest_crawl_history(self) -> Optional[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        最新のクロール履歴を取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Optional[Dict]: 最新のクロール履歴、存在しない場合はNone\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        conn.row_factory = sqlite3.Row\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT * FROM crawl_history ORDER BY id DESC LIMIT 1')\\n\",\n",
    "        \"        row = cursor.fetchone()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if row:\\n\",\n",
    "        \"            return dict(row)\\n\",\n",
    "        \"        return None\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_all_pages(self) -> List[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        すべてのキャッシュされたページ情報を取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            List[Dict]: キャッシュされたすべてのページ情報\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        conn.row_factory = sqlite3.Row\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT * FROM pages')\\n\",\n",
    "        \"        rows = cursor.fetchall()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return [dict(row) for row in rows]\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def is_content_changed(self, url: str, markdown_content: str) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        ページのコンテンツが前回のクロール時から変更されているかどうかを確認する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): チェックするページのURL\\n\",\n",
    "        \"            markdown_content (str): 現在のMarkdownコンテンツ\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            bool: コンテンツが変更されている場合はTrue、変更がない場合はFalse\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        current_hash = self._compute_hash(markdown_content)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT content_hash FROM pages WHERE url = ?', (url,))\\n\",\n",
    "        \"        row = cursor.fetchone()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if not row:\\n\",\n",
    "        \"            return True  # 新規ページなので変更ありとみなす\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return current_hash != row[0]\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _compute_hash(self, content: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        コンテンツのハッシュ値を計算する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            content (str): ハッシュ値を計算するコンテンツ\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: コンテンツのSHA256ハッシュ値\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        return hashlib.sha256(content.encode('utf-8')).hexdigest()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_diff(self, url: str, current_content: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        前回のコンテンツとの差分を取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): チェックするページのURL\\n\",\n",
    "        \"            current_content (str): 現在のMarkdownコンテンツ\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: 差分情報（unified diff形式）\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        conn = sqlite3.connect(self.db_path)\\n\",\n",
    "        \"        cursor = conn.cursor()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        cursor.execute('SELECT markdown_content FROM pages WHERE url = ?', (url,))\\n\",\n",
    "        \"        row = cursor.fetchone()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        conn.close()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if not row:\\n\",\n",
    "        \"            return \\\"新規ページ\\\"\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        old_content = row[0]\\n\",\n",
    "        \"        if not old_content:\\n\",\n",
    "        \"            return \\\"前回のコンテンツが空\\\"\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        # 差分を計算\\n\",\n",
    "        \"        diff = difflib.unified_diff(\\n\",\n",
    "        \"            old_content.splitlines(),\\n\",\n",
    "        \"            current_content.splitlines(),\\n\",\n",
    "        \"            fromfile=\\\"前回のバージョン\\\",\\n\",\n",
    "        \"            tofile=\\\"現在のバージョン\\\",\\n\",\n",
    "        \"            lineterm=''\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return '\\\\n'.join(diff)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class ContentRepository:\\n\",\n",
    "        \"    \\\"\\\"\\\"クロールしたコンテンツを管理するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        ContentRepositoryクラスの初期化\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.contents = {}  # URLをキーとしたコンテンツ辞書\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def add(self, page_data: Dict) -> None:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        コンテンツを追加する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            page_data (Dict): 追加するページデータ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        url = page_data['url']\\n\",\n",
    "        \"        self.contents[url] = page_data\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def get(self, url: str) -> Optional[Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLに対応するコンテンツを取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): 取得するコンテンツのURL\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Optional[Dict]: 取得したコンテンツ、存在しない場合はNone\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        return self.contents.get(url)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def get_all(self) -> Dict[str, Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        すべてのコンテンツを取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Dict[str, Dict]: すべてのコンテンツ（URLをキーとする辞書）\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        return self.contents\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def count(self) -> int:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        コンテンツの数を取得する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            int: コンテンツの数\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        return len(self.contents)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class FileExporter:\\n\",\n",
    "        \"    \\\"\\\"\\\"クロールしたコンテンツをファイルに出力するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, output_dir: str = \\\"output\\\"):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        FileExporterクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            output_dir (str): 出力ディレクトリ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.output_dir = output_dir\\n\",\n",
    "        \"        os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def export_markdown(self, repository: ContentRepository, filename: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        コンテンツをMarkdownファイルとしてエクスポートする\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            repository (ContentRepository): コンテンツリポジトリ\\n\",\n",
    "        \"            filename (str): 出力ファイル名\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: 出力したファイルのパス\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        contents = repository.get_all()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 出力ファイルのパス\\n\",\n",
    "        \"        output_path = os.path.join(self.output_dir, filename)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # コンテンツをリストにまとめる\\n\",\n",
    "        \"        markdown_contents = []\\n\",\n",
    "        \"        for url, page_data in sorted(contents.items()):\\n\",\n",
    "        \"            if 'markdown_content' in page_data:\\n\",\n",
    "        \"                markdown_contents.append(page_data['markdown_content'])\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        # ファイルに書き込む\\n\",\n",
    "        \"        with open(output_path, 'w', encoding='utf-8') as f:\\n\",\n",
    "        \"            f.write('\\\\n\\\\n---\\\\n\\\\n'.join(markdown_contents))\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        return output_path\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def export_diff_report(self, diff_data: Dict, filename: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        差分レポートをMarkdownファイルとして出力する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            diff_data (Dict): 差分データ\\n\",\n",
    "        \"            filename (str): 出力ファイル名\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: 出力したファイルのパス\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        output_path = os.path.join(self.output_dir, filename)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        with open(output_path, 'w', encoding='utf-8') as f:\\n\",\n",
    "        \"            f.write(f\\\"# 差分レポート - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 概要情報\\n\",\n",
    "        \"            f.write(\\\"## 概要\\\\n\\\\n\\\")\\n\",\n",
    "        \"            f.write(f\\\"- 合計ページ数: {diff_data['total']}\\\\n\\\")\\n\",\n",
    "        \"            f.write(f\\\"- 新規ページ: {len(diff_data['new_pages'])}\\\\n\\\")\\n\",\n",
    "        \"            f.write(f\\\"- 更新ページ: {len(diff_data['updated_pages'])}\\\\n\\\")\\n\",\n",
    "        \"            f.write(f\\\"- 削除ページ: {len(diff_data['deleted_pages'])}\\\\n\\\\n\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 新規ページ\\n\",\n",
    "        \"            if diff_data['new_pages']:\\n\",\n",
    "        \"                f.write(\\\"## 新規ページ\\\\n\\\\n\\\")\\n\",\n",
    "        \"                for url in diff_data['new_pages']:\\n\",\n",
    "        \"                    f.write(f\\\"- [{url}]({url})\\\\n\\\")\\n\",\n",
    "        \"                f.write(\\\"\\\\n\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 更新ページ\\n\",\n",
    "        \"            if diff_data['updated_pages']:\\n\",\n",
    "        \"                f.write(\\\"## 更新ページ\\\\n\\\\n\\\")\\n\",\n",
    "        \"                for url in diff_data['updated_pages']:\\n\",\n",
    "        \"                    f.write(f\\\"- [{url}]({url})\\\\n\\\")\\n\",\n",
    "        \"                f.write(\\\"\\\\n\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 削除ページ\\n\",\n",
    "        \"            if diff_data['deleted_pages']:\\n\",\n",
    "        \"                f.write(\\\"## 削除ページ\\\\n\\\\n\\\")\\n\",\n",
    "        \"                for url in diff_data['deleted_pages']:\\n\",\n",
    "        \"                    f.write(f\\\"- {url}\\\\n\\\")\\n\",\n",
    "        \"                f.write(\\\"\\\\n\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 詳細な差分情報 (オプション)\\n\",\n",
    "        \"            if diff_data.get('diffs'):\\n\",\n",
    "        \"                f.write(\\\"## 詳細な差分\\\\n\\\\n\\\")\\n\",\n",
    "        \"                for url, diff in diff_data['diffs'].items():\\n\",\n",
    "        \"                    f.write(f\\\"### {url}\\\\n\\\\n\\\")\\n\",\n",
    "        \"                    f.write(\\\"```diff\\\\n\\\")\\n\",\n",
    "        \"                    f.write(diff)\\n\",\n",
    "        \"                    f.write(\\\"\\\\n```\\\\n\\\\n\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return output_path\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class PdfConverter:\\n\",\n",
    "        \"    \\\"\\\"\\\"MarkdownファイルをPDF形式に変換するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, output_dir: str = \\\"output\\\"):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        PdfConverterクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            output_dir (str): 出力ディレクトリ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.output_dir = output_dir\\n\",\n",
    "        \"        os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def convert(self, markdown_path: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        MarkdownファイルをPDFに変換する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            markdown_path (str): Markdownファイルのパス\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            str: 出力したPDFファイルのパス\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        # 入力ファイル名からPDFファイル名を生成\\n\",\n",
    "        \"        pdf_filename = os.path.basename(markdown_path).replace('.md', '.pdf')\\n\",\n",
    "        \"        pdf_path = os.path.join(self.output_dir, pdf_filename)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            # Markdownを読み込む\\n\",\n",
    "        \"            with open(markdown_path, 'r', encoding='utf-8') as md_file:\\n\",\n",
    "        \"                md_content = md_file.read()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # MarkdownをHTML形式に変換\\n\",\n",
    "        \"            html_content = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # HTMLをPDFに変換\\n\",\n",
    "        \"            html_path = os.path.join(self.output_dir, \\\"temp.html\\\")\\n\",\n",
    "        \"            with open(html_path, 'w', encoding='utf-8') as f:\\n\",\n",
    "        \"                f.write(f\\\"<html><head><meta charset='utf-8'></head><body>{html_content}</body></html>\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Google Colab用の設定（パスを指定）\\n\",\n",
    "        \"            config = pdfkit.configuration(wkhtmltopdf='/usr/bin/wkhtmltopdf')\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # wkhtmltopdfを使用してPDFに変換\\n\",\n",
    "        \"            pdfkit.from_file(html_path, pdf_path, configuration=config)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 一時ファイルを削除\\n\",\n",
    "        \"            if os.path.exists(html_path):\\n\",\n",
    "        \"                os.remove(html_path)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            return pdf_path\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error converting to PDF: {e}\\\")\\n\",\n",
    "        \"            return None\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class DiscordNotifier:\\n\",\n",
    "        \"    \\\"\\\"\\\"Discordに通知を送信するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, webhook_url: str):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        DiscordNotifierクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            webhook_url (str): Discord Webhook URL\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.webhook_url = webhook_url\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def notify(self, message: str, markdown_path: Optional[str] = None, pdf_path: Optional[str] = None) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Discord通知を送信する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            message (str): 通知メッセージ\\n\",\n",
    "        \"            markdown_path (Optional[str]): 添付するMarkdownファイルのパス\\n\",\n",
    "        \"            pdf_path (Optional[str]): 添付するPDFファイルのパス\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            bool: 通知が成功した場合はTrue、失敗した場合はFalse\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            # Webhookインスタンスを作成\\n\",\n",
    "        \"            webhook = DiscordWebhook(url=self.webhook_url, content=message)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Markdownファイルを添付\\n\",\n",
    "        \"            if markdown_path and os.path.exists(markdown_path):\\n\",\n",
    "        \"                with open(markdown_path, 'rb') as f:\\n\",\n",
    "        \"                    webhook.add_file(file=f.read(), filename=os.path.basename(markdown_path))\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # PDFファイルを添付\\n\",\n",
    "        \"            if pdf_path and os.path.exists(pdf_path):\\n\",\n",
    "        \"                with open(pdf_path, 'rb') as f:\\n\",\n",
    "        \"                    webhook.add_file(file=f.read(), filename=os.path.basename(pdf_path))\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 通知を送信\\n\",\n",
    "        \"            response = webhook.execute()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # レスポンスコードをチェック\\n\",\n",
    "        \"            if response and 200 <= response.status_code < 300:\\n\",\n",
    "        \"                logging.info(\\\"Discord notification sent successfully\\\")\\n\",\n",
    "        \"                return True\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                logging.error(f\\\"Failed to send Discord notification: {response.status_code if response else 'No response'}\\\")\\n\",\n",
    "        \"                return False\\n\",\n",
    "        \"                \\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error sending Discord notification: {e}\\\")\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class RobotsTxtParser:\\n\",\n",
    "        \"    \\\"\\\"\\\"robots.txtを解析してクロール許可を確認するコンポーネント\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, base_url: str, user_agent: str = \\\"*\\\"):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        RobotsTxtParserクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            base_url (str): ベースURL\\n\",\n",
    "        \"            user_agent (str): User-Agent文字列\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.base_url = base_url\\n\",\n",
    "        \"        self.user_agent = user_agent\\n\",\n",
    "        \"        self.disallowed_paths = []\\n\",\n",
    "        \"        self.crawl_delay = 0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        parsed_url = urlparse(base_url)\\n\",\n",
    "        \"        robots_url = f\\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\\"\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            response = requests.get(robots_url, timeout=10)\\n\",\n",
    "        \"            if response.status_code == 200:\\n\",\n",
    "        \"                self._parse_robots_txt(response.text)\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                logging.warning(f\\\"Could not fetch robots.txt: {response.status_code}\\\")\\n\",\n",
    "        \"        except requests.RequestException as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error fetching robots.txt: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def _parse_robots_txt(self, robots_txt: str) -> None:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        robots.txtの内容を解析する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            robots_txt (str): robots.txtの内容\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        current_agent = None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for line in robots_txt.split('\\\\n'):\\n\",\n",
    "        \"            line = line.strip().lower()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            if not line or line.startswith('#'):\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            parts = line.split(':', 1)\\n\",\n",
    "        \"            if len(parts) != 2:\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            directive, value = parts\\n\",\n",
    "        \"            directive = directive.strip()\\n\",\n",
    "        \"            value = value.strip()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            if directive == 'user-agent':\\n\",\n",
    "        \"                current_agent = value\\n\",\n",
    "        \"            elif current_agent in (self.user_agent, '*') and directive == 'disallow' and value:\\n\",\n",
    "        \"                self.disallowed_paths.append(value)\\n\",\n",
    "        \"            elif current_agent in (self.user_agent, '*') and directive == 'crawl-delay':\\n\",\n",
    "        \"                try:\\n\",\n",
    "        \"                    self.crawl_delay = float(value)\\n\",\n",
    "        \"                except ValueError:\\n\",\n",
    "        \"                    pass\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def is_allowed(self, url: str) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        URLがrobots.txtによりクロールを許可されているかを確認する\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            url (str): 確認するURL\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            bool: クロールが許可されている場合はTrue、禁止されている場合はFalse\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        parsed_url = urlparse(url)\\n\",\n",
    "        \"        path = parsed_url.path\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        for disallowed in self.disallowed_paths:\\n\",\n",
    "        \"            if path.startswith(disallowed):\\n\",\n",
    "        \"                return False\\n\",\n",
    "        \"                \\n\",\n",
    "        \"        return True\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class WebCrawler:\\n\",\n",
    "        \"    \\\"\\\"\\\"Webクローラーのメインコントローラー\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, base_url: str, max_pages: int = 100, delay: float = 1.0, diff_detection: bool = True, cache_dir: str = \\\"cache\\\"):\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        WebCrawlerクラスの初期化\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Args:\\n\",\n",
    "        \"            base_url (str): クロールを開始するURL\\n\",\n",
    "        \"            max_pages (int): クロールする最大ページ数\\n\",\n",
    "        \"            delay (float): リクエスト間の遅延秒数\\n\",\n",
    "        \"            diff_detection (bool): 差分検知を有効にするかどうか\\n\",\n",
    "        \"            cache_dir (str): キャッシュディレクトリ\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        self.base_url = base_url\\n\",\n",
    "        \"        self.max_pages = max_pages\\n\",\n",
    "        \"        self.diff_detection = diff_detection\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # ドメイン名を取得\\n\",\n",
    "        \"        self.domain = urlparse(base_url).netloc\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 各コンポーネントの初期化\\n\",\n",
    "        \"        self.url_filter = UrlFilter(base_url)\\n\",\n",
    "        \"        self.robots_parser = RobotsTxtParser(base_url)\\n\",\n",
    "        \"        self.fetcher = Fetcher(delay=max(delay, self.robots_parser.crawl_delay))\\n\",\n",
    "        \"        self.parser = Parser(self.url_filter)\\n\",\n",
    "        \"        self.markdown_converter = MarkdownConverter()\\n\",\n",
    "        \"        self.repository = ContentRepository()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if self.diff_detection:\\n\",\n",
    "        \"            self.cache = CrawlCache(self.domain, cache_dir)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # クロール状態の追跡\\n\",\n",
    "        \"        self.visited_urls = set()\\n\",\n",
    "        \"        self.queue = deque([base_url])\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 差分情報の追跡\\n\",\n",
    "        \"        self.new_pages = []\\n\",\n",
    "        \"        self.updated_pages = []\\n\",\n",
    "        \"        self.deleted_pages = []\\n\",\n",
    "        \"        self.page_diffs = {}\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    def crawl(self) -> Tuple[ContentRepository, Dict]:\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        Webサイトをクロールする\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        Returns:\\n\",\n",
    "        \"            Tuple[ContentRepository, Dict]: (クロールしたコンテンツのリポジトリ, 差分情報)\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"        count = 0\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        while self.queue and count < self.max_pages:\\n\",\n",
    "        \"            # キューからURLを取得\\n\",\n",
    "        \"            url = self.queue.popleft()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 既に訪問済みのURLはスキップ\\n\",\n",
    "        \"            if url in self.visited_urls:\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # robots.txtで禁止されているURLはスキップ\\n\",\n",
    "        \"            if not self.robots_parser.is_allowed(url):\\n\",\n",
    "        \"                logging.info(f\\\"Skipping URL disallowed by robots.txt: {url}\\\")\\n\",\n",
    "        \"                self.visited_urls.add(url)\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            logging.info(f\\\"Crawling {url} ({count + 1}/{self.max_pages})\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # キャッシュからページ情報を取得\\n\",\n",
    "        \"            cached_page = None\\n\",\n",
    "        \"            if self.diff_detection:\\n\",\n",
    "        \"                cached_page = self.cache.get_page(url)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # ページのHTMLを取得（条件付きリクエスト）\\n\",\n",
    "        \"            etag = cached_page.get('etag') if cached_page else None\\n\",\n",
    "        \"            last_modified = cached_page.get('last_modified') if cached_page else None\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            html, headers_info = self.fetcher.fetch(url, etag, last_modified)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 304 Not Modified の場合、キャッシュから前回のコンテンツを使用\\n\",\n",
    "        \"            if headers_info.get('status_code') == 304 and cached_page:\\n\",\n",
    "        \"                logging.info(f\\\"Using cached content for {url}\\\")\\n\",\n",
    "        \"                page_data = {\\n\",\n",
    "        \"                    'url': url,\\n\",\n",
    "        \"                    'title': cached_page['title'],\\n\",\n",
    "        \"                    'html_content': '', # HTMLは保存不要\\n\",\n",
    "        \"                    'markdown_content': cached_page['markdown_content'],\\n\",\n",
    "        \"                    'etag': cached_page['etag'],\\n\",\n",
    "        \"                    'last_modified': cached_page['last_modified'],\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"                self.repository.add(page_data)\\n\",\n",
    "        \"                self.visited_urls.add(url)\\n\",\n",
    "        \"                count += 1\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # HTMLが取得できなかった場合はスキップ\\n\",\n",
    "        \"            if html is None:\\n\",\n",
    "        \"                self.visited_urls.add(url)\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # HTMLを解析してコンテンツとリンクを抽出\\n\",\n",
    "        \"            page_data, links = self.parser.parse(html, url)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # コンテンツがない場合はスキップ\\n\",\n",
    "        \"            if not page_data.get('html_content'):\\n\",\n",
    "        \"                self.visited_urls.add(url)\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # ヘッダー情報を追加\\n\",\n",
    "        \"            page_data['etag'] = headers_info.get('etag')\\n\",\n",
    "        \"            page_data['last_modified'] = headers_info.get('last_modified')\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # HTMLをMarkdownに変換\\n\",\n",
    "        \"            page_data = self.markdown_converter.convert(page_data)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 差分検知（有効な場合）\\n\",\n",
    "        \"            if self.diff_detection:\\n\",\n",
    "        \"                markdown_content = page_data.get('markdown_content', '')\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # キャッシュに追加または更新\\n\",\n",
    "        \"                is_update = self.cache.add_or_update_page(page_data)\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                if is_update:\\n\",\n",
    "        \"                    # コンテンツが変更されている場合のみ更新ページとしてマーク\\n\",\n",
    "        \"                    if self.cache.is_content_changed(url, markdown_content):\\n\",\n",
    "        \"                        self.updated_pages.append(url)\\n\",\n",
    "        \"                        self.page_diffs[url] = self.cache.get_diff(url, markdown_content)\\n\",\n",
    "        \"                else:\\n\",\n",
    "        \"                    # 新規ページ\\n\",\n",
    "        \"                    self.new_pages.append(url)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # コンテンツを保存\\n\",\n",
    "        \"            self.repository.add(page_data)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 訪問済みとしてマーク\\n\",\n",
    "        \"            self.visited_urls.add(url)\\n\",\n",
    "        \"            count += 1\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 新しいリンクをキューに追加\\n\",\n",
    "        \"            for link in links:\\n\",\n",
    "        \"                if link not in self.visited_urls and link not in self.queue:\\n\",\n",
    "        \"                    self.queue.append(link)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 削除されたページを特定（差分検知が有効な場合）\\n\",\n",
    "        \"        if self.diff_detection:\\n\",\n",
    "        \"            cached_urls = self.cache.get_all_urls()\\n\",\n",
    "        \"            current_urls = set(self.repository.get_all().keys())\\n\",\n",
    "        \"            self.deleted_pages = list(cached_urls - current_urls)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 削除されたページをキャッシュから削除\\n\",\n",
    "        \"            if self.deleted_pages:\\n\",\n",
    "        \"                self.cache.delete_urls(self.deleted_pages)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # クロール履歴を保存\\n\",\n",
    "        \"            self.cache.save_crawl_history(\\n\",\n",
    "        \"                page_count=self.repository.count(),\\n\",\n",
    "        \"                new_count=len(self.new_pages),\\n\",\n",
    "        \"                updated_count=len(self.updated_pages),\\n\",\n",
    "        \"                deleted_count=len(self.deleted_pages)\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 差分情報を作成\\n\",\n",
    "        \"        diff_data = {\\n\",\n",
    "        \"            'total': self.repository.count(),\\n\",\n",
    "        \"            'new_pages': self.new_pages,\\n\",\n",
    "        \"            'updated_pages': self.updated_pages,\\n\",\n",
    "        \"            'deleted_pages': self.deleted_pages,\\n\",\n",
    "        \"            'diffs': self.page_diffs,\\n\",\n",
    "        \"            'has_changes': bool(self.new_pages or self.updated_pages or self.deleted_pages)\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        logging.info(f\\\"Crawling completed. Visited {len(self.visited_urls)} URLs, stored {self.repository.count()} pages.\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Changes detected: {len(self.new_pages)} new, {len(self.updated_pages)} updated, {len(self.deleted_pages)} deleted.\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return self.repository, diff_data\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def run_crawler(url, max_pages=100, delay=1.0, output_dir=\\\"output\\\", cache_dir=\\\"cache\\\", discord_webhook=None, no_diff=False, skip_no_changes=False):\\n\",\n",
    "        \"    \\\"\\\"\\\"Google Colab向けのクローラー実行関数\\\"\\\"\\\"\\n\",\n",
    "        \"    # ロガーの設定\\n\",\n",
    "        \"    logging.basicConfig(\\n\",\n",
    "        \"        level=logging.INFO,\\n\",\n",
    "        \"        format='%(asctime)s - %(levelname)s - %(message)s',\\n\",\n",
    "        \"        handlers=[\\n\",\n",
    "        \"            logging.FileHandler(os.path.join(output_dir, \\\"crawler.log\\\")),\\n\",\n",
    "        \"            logging.StreamHandler()\\n\",\n",
    "        \"        ]\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        # 出力ディレクトリを作成\\n\",\n",
    "        \"        os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # クローラーの初期化と実行\\n\",\n",
    "        \"        crawler = WebCrawler(url, max_pages, delay, diff_detection=not no_diff, cache_dir=cache_dir)\\n\",\n",
    "        \"        repository, diff_data = crawler.crawl()\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 結果がない場合はエラー\\n\",\n",
    "        \"        if repository.count() == 0:\\n\",\n",
    "        \"            logging.error(\\\"No content was crawled.\\\")\\n\",\n",
    "        \"            if discord_webhook:\\n\",\n",
    "        \"                notifier = DiscordNotifier(discord_webhook)\\n\",\n",
    "        \"                notifier.notify(message=f\\\"Webサイトのクロールが完了しましたが、コンテンツは取得できませんでした。\\\\n**URL**: {url}\\\")\\n\",\n",
    "        \"            return None, None, None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 変更がなく、スキップオプションが有効な場合はスキップ\\n\",\n",
    "        \"        if skip_no_changes and not diff_data['has_changes']:\\n\",\n",
    "        \"            logging.info(\\\"No changes detected. Skipping file generation and notification.\\\")\\n\",\n",
    "        \"            if discord_webhook:\\n\",\n",
    "        \"                notifier = DiscordNotifier(discord_webhook)\\n\",\n",
    "        \"                notifier.notify(message=f\\\"Webサイトのクロールが完了しましたが、前回から変更はありませんでした。\\\\n**URL**: {url}\\\\n**取得ページ数**: {repository.count()}\\\")\\n\",\n",
    "        \"            return None, None, None\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # ドメイン名をファイル名として使用\\n\",\n",
    "        \"        domain = urlparse(url).netloc\\n\",\n",
    "        \"        markdown_filename = f\\\"{domain}.md\\\"\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Markdownファイルとして出力\\n\",\n",
    "        \"        exporter = FileExporter(output_dir)\\n\",\n",
    "        \"        markdown_path = exporter.export_markdown(repository, markdown_filename)\\n\",\n",
    "        \"        logging.info(f\\\"Exported Markdown to {markdown_path}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 差分レポートを出力（差分検知が有効な場合）\\n\",\n",
    "        \"        diff_report_path = None\\n\",\n",
    "        \"        if not no_diff and diff_data['has_changes']:\\n\",\n",
    "        \"            diff_report_filename = f\\\"{domain}_diff_report.md\\\"\\n\",\n",
    "        \"            diff_report_path = exporter.export_diff_report(diff_data, diff_report_filename)\\n\",\n",
    "        \"            logging.info(f\\\"Exported diff report to {diff_report_path}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # PDFファイルとして出力\\n\",\n",
    "        \"        pdf_converter = PdfConverter(output_dir)\\n\",\n",
    "        \"        pdf_path = pdf_converter.convert(markdown_path)\\n\",\n",
    "        \"        if pdf_path:\\n\",\n",
    "        \"            logging.info(f\\\"Exported PDF to {pdf_path}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 差分レポートのPDFを生成（差分がある場合）\\n\",\n",
    "        \"        diff_report_pdf_path = None\\n\",\n",
    "        \"        if diff_report_path:\\n\",\n",
    "        \"            diff_report_pdf_path = pdf_converter.convert(diff_report_path)\\n\",\n",
    "        \"            if diff_report_pdf_path:\\n\",\n",
    "        \"                logging.info(f\\\"Exported diff report PDF to {diff_report_pdf_path}\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Discord通知\\n\",\n",
    "        \"        if discord_webhook:\\n\",\n",
    "        \"            notifier = DiscordNotifier(discord_webhook)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # 差分検知が有効かつ変更がある場合\\n\",\n",
    "        \"            if not no_diff and diff_data['has_changes']:\\n\",\n",
    "        \"                message = f\\\"Webサイトのクロールが完了しました。**変更が検出されました**。\\\\n\\\"\\n\",\n",
    "        \"                message += f\\\"**URL**: {url}\\\\n\\\"\\n\",\n",
    "        \"                message += f\\\"**取得ページ数**: {diff_data['total']}\\\\n\\\"\\n\",\n",
    "        \"                message += f\\\"**新規ページ**: {len(diff_data['new_pages'])}\\\\n\\\"\\n\",\n",
    "        \"                message += f\\\"**更新ページ**: {len(diff_data['updated_pages'])}\\\\n\\\"\\n\",\n",
    "        \"                message += f\\\"**削除ページ**: {len(diff_data['deleted_pages'])}\\\"\\n\",\n",
    "        \"                \\n\",\n",
    "        \"                # 差分レポートを添付\\n\",\n",
    "        \"                success = notifier.notify(\\n\",\n",
    "        \"                    message=message,\\n\",\n",
    "        \"                    markdown_path=diff_report_path,\\n\",\n",
    "        \"                    pdf_path=diff_report_pdf_path or pdf_path\\n\",\n",
    "        \"                )\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                # 変更がない場合または差分検知が無効の場合\\n\",\n",
    "        \"                message = f\\\"Webサイトのクロールが完了しました。\\\\n**URL**: {url}\\\\n**取得ページ数**: {repository.count()}\\\"\\n\",\n",
    "        \"                success = notifier.notify(\\n\",\n",
    "        \"                    message=message,\\n\",\n",
    "        \"                    markdown_path=markdown_path,\\n\",\n",
    "        \"                    pdf_path=pdf_path\\n\",\n",
    "        \"                )\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            if success:\\n\",\n",
    "        \"                logging.info(\\\"Discord notification sent successfully\\\")\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                logging.error(\\\"Failed to send Discord notification\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        logging.info(\\\"Process completed successfully\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        return markdown_path, pdf_path, diff_report_path\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"An error occurred during execution: {e}\\\")\\n\",\n",
    "        \"        if discord_webhook:\\n\",\n",
    "        \"            notifier = DiscordNotifier(discord_webhook)\\n\",\n",
    "        \"            notifier.notify(message=f\\\"Webサイトのクロール中にエラーが発生しました。\\\\n**URL**: {url}\\\\n**エラー**: {str(e)}\\\")\\n\",\n",
    "        \"        return None, None, None\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"user-interface\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 4. Google Colab用のユーザーインターフェース\\n\",\n",
    "        \"\\n\",\n",
    "        \"以下のセルを実行して、クローラーの実行パラメータを設定します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"form-cell\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"from IPython.display import display, HTML, Javascript, FileLink\\n\",\n",
    "        \"import ipywidgets as widgets\\n\",\n",
    "        \"\\n\",\n",
    "        \"# URL入力フィールド\\n\",\n",
    "        \"url_input = widgets.Text(\\n\",\n",
    "        \"    value='https://example.com',\\n\",\n",
    "        \"    placeholder='クロールするWebサイトのURLを入力してください',\\n\",\n",
    "        \"    description='URL:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 最大ページ数スライダー\\n\",\n",
    "        \"max_pages_slider = widgets.IntSlider(\\n\",\n",
    "        \"    value=100,\\n\",\n",
    "        \"    min=10,\\n\",\n",
    "        \"    max=500,\\n\",\n",
    "        \"    step=10,\\n\",\n",
    "        \"    description='最大ページ数:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    continuous_update=False,\\n\",\n",
    "        \"    orientation='horizontal',\\n\",\n",
    "        \"    readout=True,\\n\",\n",
    "        \"    readout_format='d'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 遅延時間スライダー\\n\",\n",
    "        \"delay_slider = widgets.FloatSlider(\\n\",\n",
    "        \"    value=1.0,\\n\",\n",
    "        \"    min=0.5,\\n\",\n",
    "        \"    max=5.0,\\n\",\n",
    "        \"    step=0.5,\\n\",\n",
    "        \"    description='遅延時間(秒):',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    continuous_update=False,\\n\",\n",
    "        \"    orientation='horizontal',\\n\",\n",
    "        \"    readout=True,\\n\",\n",
    "        \"    readout_format='.1f'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Discord Webhook URL入力\\n\",\n",
    "        \"discord_webhook_input = widgets.Text(\\n\",\n",
    "        \"    value='',\\n\",\n",
    "        \"    placeholder='Discord Webhook URLを入力してください（オプション）',\\n\",\n",
    "        \"    description='Discord:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# オプションチェックボックス\\n\",\n",
    "        \"diff_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='差分検知を有効にする',\\n\",\n",
    "        \"    disabled=False\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"skip_no_changes_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='変更がない場合はスキップ',\\n\",\n",
    "        \"    disabled=False\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# パス設定\\n\",\n",
    "        \"output_dir = widgets.Text(\\n\",\n",
    "        \"    value='/content/drive/MyDrive/website_crawler/output',\\n\",\n",
    "        \"    description='出力ディレクトリ:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"cache_dir = widgets.Text(\\n\",\n",
    "        \"    value='/content/drive/MyDrive/website_crawler/cache',\\n\",\n",
    "        \"    description='キャッシュディレクトリ:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 実行ボタン\\n\",\n",
    "        \"run_button = widgets.Button(\\n\",\n",
    "        \"    description='クローラーを実行',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    button_style='success',\\n\",\n",
    "        \"    tooltip='クリックしてクローラーを実行',\\n\",\n",
    "        \"    icon='play'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 出力エリア\\n\",\n",
    "        \"output = widgets.Output()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ボタンクリックイベント\\n\",\n",
    "        \"def on_run_button_clicked(b):\\n\",\n",
    "        \"    with output:\\n\",\n",
    "        \"        output.clear_output()\\n\",\n",
    "        \"        print(\\\"クローラーを実行中...\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        url = url_input.value\\n\",\n",
    "        \"        max_pages = max_pages_slider.value\\n\",\n",
    "        \"        delay = delay_slider.value\\n\",\n",
    "        \"        discord_webhook = discord_webhook_input.value if discord_webhook_input.value else None\\n\",\n",
    "        \"        no_diff = not diff_checkbox.value\\n\",\n",
    "        \"        skip_no_changes = skip_no_changes_checkbox.value\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # クローラーを実行\\n\",\n",
    "        \"        markdown_path, pdf_path, diff_path = run_crawler(\\n\",\n",
    "        \"            url=url,\\n\",\n",
    "        \"            max_pages=max_pages,\\n\",\n",
    "        \"            delay=delay,\\n\",\n",
    "        \"            output_dir=output_dir.value,\\n\",\n",
    "        \"            cache_dir=cache_dir.value,\\n\",\n",
    "        \"            discord_webhook=discord_webhook,\\n\",\n",
    "        \"            no_diff=no_diff,\\n\",\n",
    "        \"            skip_no_changes=skip_no_changes\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 結果を表示\\n\",\n",
    "        \"        if markdown_path:\\n\",\n",
    "        \"            print(f\\\"\\\\n処理が完了しました！\\\")\\n\",\n",
    "        \"            print(f\\\"\\\\nMarkdownファイル: {markdown_path}\\\")\\n\",\n",
    "        \"            if pdf_path:\\n\",\n",
    "        \"                print(f\\\"PDFファイル: {pdf_path}\\\")\\n\",\n",
    "        \"            if diff_path:\\n\",\n",
    "        \"                print(f\\\"差分レポート: {diff_path}\\\")\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            # ファイルへのリンクを表示\\n\",\n",
    "        \"            if os.path.exists(markdown_path):\\n\",\n",
    "        \"                print(\\\"\\\\nファイルをダウンロード:\\\")\\n\",\n",
    "        \"                display(FileLink(markdown_path))\\n\",\n",
    "        \"                if pdf_path and os.path.exists(pdf_path):\\n\",\n",
    "        \"                    display(FileLink(pdf_path))\\n\",\n",
    "        \"                if diff_path and os.path.exists(diff_path):\\n\",\n",
    "        \"                    display(FileLink(diff_path))\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            print(\\\"\\\\nエラーが発生したかクロールをスキップしました。ログを確認してください。\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"run_button.on_click(on_run_button_clicked)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# UIを表示\\n\",\n",
    "        \"display(widgets.HTML(\\\"<h3>Webサイトクローラー設定</h3>\\\"))\\n\",\n",
    "        \"display(url_input)\\n\",\n",
    "        \"display(widgets.HBox([max_pages_slider, delay_slider]))\\n\",\n",
    "        \"display(discord_webhook_input)\\n\",\n",
    "        \"display(widgets.HBox([diff_checkbox, skip_no_changes_checkbox]))\\n\",\n",
    "        \"display(output_dir)\\n\",\n",
    "        \"display(cache_dir)\\n\",\n",
    "        \"display(run_button)\\n\",\n",
    "        \"display(output)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"manual-run\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 5. 手動でクローラーを実行する（オプション）\\n\",\n",
    "        \"\\n\",\n",
    "        \"必要に応じて、以下のセルでクローラーを直接呼び出すこともできます。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"manual-run-code\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# 手動実行の例\\n\",\n",
    "        \"# markdown_path, pdf_path, diff_path = run_crawler(\\n\",\n",
    "        \"#     url=\\\"https://example.com\\\",\\n\",\n",
    "        \"#     max_pages=100,\\n\",\n",
    "        \"#     delay=1.0,\\n\",\n",
    "        \"#     output_dir=crawler_dir + \\\"/output\\\",\\n\",\n",
    "        \"#     cache_dir=crawler_dir + \\\"/cache\\\",\\n\",\n",
    "        \"#     discord_webhook=None,  # ここにWebhook URLを入力\\n\",\n",
    "        \"#     no_diff=False,\\n\",\n",
    "        \"#     skip_no_changes=True\\n\",\n",
    "        \"# )\\n\",\n",
    "        \"# \\n\",\n",
    "        \"# print(\\\"処理結果:\\\")\\n\",\n",
    "        \"# print(f\\\"Markdown: {markdown_path}\\\")\\n\",\n",
    "        \"# print(f\\\"PDF: {pdf_path}\\\")\\n\",\n",
    "        \"# print(f\\\"差分レポート: {diff_path}\\\")\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"schedule-note\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 6. 定期実行について\\n\",\n",
    "        \"\\n\",\n",
    "        \"Google Colabでは、セッションが一定時間後に切断されるため、長時間の定期実行には向いていません。\\n\",\n",
    "        \"定期的なクロールを行いたい場合は、以下の選択肢があります：\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. ローカルマシンでPythonスクリプトとして実行する\\n\",\n",
    "        \"2. Google Cloud FunctionsやCloud Runなどのサーバーレスサービスで定期実行する\\n\",\n",
    "        \"3. GitHub ActionsやCircle CIなどのCI/CDサービスを利用して定期実行する\\n\",\n",
    "        \"\\n\",\n",
    "        \"このノートブックはあくまで手動実行用として使用してください。\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
