{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"view-in-github\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# 改善版 Webサイトクローラー（差分検知機能付き）- Google Colab版\\n\",\n",
    "        \"\\n\",\n",
    "        \"指定したURLから始めて同一ドメイン内のすべてのページをクロールし、Markdown形式で出力します。\\n\",\n",
    "        \"さらに、前回のクロール結果との差分を検出し、レポートを生成します。\\n\",\n",
    "        \"\\n\",\n",
    "        \"## 機能\\n\",\n",
    "        \"- 同一ドメイン内のページのみをクロール\\n\",\n",
    "        \"- HTMLコンテンツをMarkdownに変換して出力\\n\",\n",
    "        \"- MarkdownをPDFに変換\\n\",\n",
    "        \"- 前回クロール結果との差分を検出（新規/更新/削除ページ）\\n\",\n",
    "        \"- Discord通知機能\\n\",\n",
    "        \"- Google Drive連携（結果を保存）\\n\",\n",
    "        \"- 非同期・並列処理による高速化\\n\",\n",
    "        \"- サイトマップXML自動生成\\n\",\n",
    "        \"- 詳細なエラーハンドリング\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"setup-section\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 1. 必要なライブラリのインストール\\n\",\n",
    "        \"\\n\",\n",
    "        \"最初に必要なライブラリをインストールします。PDF生成のためのwkhtmltopdfも導入します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 1,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"install-libraries\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"!apt-get update\\n\",\n",
    "        \"!apt-get install -y wkhtmltopdf\\n\",\n",
    "        \"!pip install requests html2text lxml markdown pdfkit discord-webhook aiohttp dataclasses-json ipywidgets\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"drive-mount\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 2. Google Driveのマウント\\n\",\n",
    "        \"\\n\",\n",
    "        \"クロール結果やキャッシュを永続的に保存するために、Google Driveをマウントします。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 2,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"mount-drive-code\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from google.colab import drive\\n\",\n",
    "        \"drive.mount('/content/drive')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# クローラー用のディレクトリを作成\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"crawler_dir = '/content/drive/MyDrive/website_crawler'\\n\",\n",
    "        \"output_dir = os.path.join(crawler_dir, 'output')\\n\",\n",
    "        \"cache_dir = os.path.join(crawler_dir, 'cache')\\n\",\n",
    "        \"\\n\",\n",
    "        \"os.makedirs(crawler_dir, exist_ok=True)\\n\",\n",
    "        \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"クローラーディレクトリを作成しました: {crawler_dir}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"components-definition\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 3. クローラーコンポーネントの定義\\n\",\n",
    "        \"\\n\",\n",
    "        \"クローラーの主要コンポーネントを定義します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 3,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"crawler-components\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# crawler_components.py - クローラーの基本コンポーネント\\n\",\n",
    "        \"\\n\",\n",
    "        \"%%writefile crawler_components.py\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\\"\\\"\\\"再利用可能なWebクローラーコンポーネント (重要なクラスのみ表示)\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import hashlib\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"import sqlite3\\n\",\n",
    "        \"import asyncio\\n\",\n",
    "        \"import requests\\n\",\n",
    "        \"import html2text\\n\",\n",
    "        \"import markdown\\n\",\n",
    "        \"from urllib.parse import urlparse, urljoin, parse_qs, urlencode\\n\",\n",
    "        \"from typing import Set, Dict, List, Optional, Tuple, Any, Union, Callable\\n\",\n",
    "        \"from datetime import datetime\\n\",\n",
    "        \"import difflib\\n\",\n",
    "        \"import lxml.html\\n\",\n",
    "        \"from concurrent.futures import ThreadPoolExecutor\\n\",\n",
    "        \"from dataclasses import dataclass, field\\n\",\n",
    "        \"from contextlib import contextmanager\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 設定クラス\\n\",\n",
    "        \"@dataclass\\n\",\n",
    "        \"class CrawlerConfig:\\n\",\n",
    "        \"    \\\"\\\"\\\"クローラーの設定を管理するクラス\\\"\\\"\\\"\\n\",\n",
    "        \"    base_url: str\\n\",\n",
    "        \"    max_pages: int = 100\\n\",\n",
    "        \"    delay: float = 1.0\\n\",\n",
    "        \"    user_agent: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\\n\",\n",
    "        \"    timeout: int = 10\\n\",\n",
    "        \"    max_retries: int = 3\\n\",\n",
    "        \"    max_workers: int = 5  # 並列実行用のワーカー数\\n\",\n",
    "        \"    output_dir: str = \\\"output\\\"\\n\",\n",
    "        \"    cache_dir: str = \\\"cache\\\"\\n\",\n",
    "        \"    discord_webhook: Optional[str] = None\\n\",\n",
    "        \"    diff_detection: bool = True\\n\",\n",
    "        \"    skip_no_changes: bool = True\\n\",\n",
    "        \"    normalize_urls: bool = True  # URL正規化の有効化\\n\",\n",
    "        \"    respect_robots_txt: bool = True  # robots.txtの尊重\\n\",\n",
    "        \"    follow_redirects: bool = True  # リダイレクトの追跡\\n\",\n",
    "        \"    static_extensions: Set[str] = field(default_factory=lambda: {\\n\",\n",
    "        \"        '.jpg', '.jpeg', '.png', '.gif', '.svg', '.css',\\n\",\n",
    "        \"        '.js', '.pdf', '.zip', '.tar', '.gz', '.mp3',\\n\",\n",
    "        \"        '.mp4', '.avi', '.mov', '.webm', '.webp', '.ico'\\n\",\n",
    "        \"    })\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    @classmethod\\n\",\n",
    "        \"    def from_dict(cls, config_dict: Dict[str, Any]) -> 'CrawlerConfig':\\n\",\n",
    "        \"        \\\"\\\"\\\"辞書から設定オブジェクトを作成する\\\"\\\"\\\"\\n\",\n",
    "        \"        return cls(**{k: v for k, v in config_dict.items() if k in cls.__annotations__})\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def to_dict(self) -> Dict[str, Any]:\\n\",\n",
    "        \"        \\\"\\\"\\\"設定を辞書に変換する\\\"\\\"\\\"\\n\",\n",
    "        \"        return {k: v for k, v in self.__dict__.items()}\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    @classmethod\\n\",\n",
    "        \"    def from_json(cls, json_path: str) -> 'CrawlerConfig':\\n\",\n",
    "        \"        \\\"\\\"\\\"JSONファイルから設定オブジェクトを作成する\\\"\\\"\\\"\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            with open(json_path, 'r', encoding='utf-8') as f:\\n\",\n",
    "        \"                config_dict = json.load(f)\\n\",\n",
    "        \"            return cls.from_dict(config_dict)\\n\",\n",
    "        \"        except (FileNotFoundError, json.JSONDecodeError) as e:\\n\",\n",
    "        \"            logging.error(f\\\"設定ファイルの読み込みに失敗しました: {e}\\\")\\n\",\n",
    "        \"            raise\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class UrlFilter:\\n\",\n",
    "        \"    \\\"\\\"\\\"URLをフィルタリングして、同一ドメイン内のURLのみを許可するコンポーネント（改善版）\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, config: CrawlerConfig):\\n\",\n",
    "        \"        \\\"\\\"\\\"URLフィルタークラスの初期化\\\"\\\"\\\"\\n\",\n",
    "        \"        self.base_url = config.base_url\\n\",\n",
    "        \"        self.base_domain = urlparse(config.base_url).netloc\\n\",\n",
    "        \"        self.static_extensions = config.static_extensions\\n\",\n",
    "        \"        self.normalize_urls = config.normalize_urls\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 除外パターンの正規表現（オプション）\\n\",\n",
    "        \"        self.exclude_patterns = [\\n\",\n",
    "        \"            r'\\\\/(?:calendar|login|logout|signup|register|password-reset)(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/feed(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/wp-admin(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/wp-content\\\\/(?:cache|uploads)(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/cart(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/checkout(?:\\\\/|$)',\\n\",\n",
    "        \"            r'\\\\/my-account(?:\\\\/|$)',\\n\",\n",
    "        \"        ]\\n\",\n",
    "        \"        self.exclude_regex = re.compile('|'.join(self.exclude_patterns))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def normalize_url(self, url: str) -> str:\\n\",\n",
    "        \"        \\\"\\\"\\\"URLを正規化する\\\"\\\"\\\"\\n\",\n",
    "        \"        # 相対URLを絶対URLに変換\\n\",\n",
    "        \"        normalized_url = urljoin(self.base_url, url)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # フラグメント (#) を削除\\n\",\n",
    "        \"        normalized_url = normalized_url.split('#')[0]\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if self.normalize_urls:\\n\",\n",
    "        \"            # クエリパラメータを正規化（オプション）\\n\",\n",
    "        \"            parsed = urlparse(normalized_url)\\n\",\n",
    "        \"            if parsed.query:\\n\",\n",
    "        \"                # クエリパラメータを正規化：アルファベット順にソート\\n\",\n",
    "        \"                params = parse_qs(parsed.query)\\n\",\n",
    "        \"                # UTM系パラメータなど、特定のトラッキングパラメータを除外\\n\",\n",
    "        \"                for param in list(params.keys()):\\n\",\n",
    "        \"                    if param.startswith('utm_') or param in ['fbclid', 'gclid', 'ref']:\\n\",\n",
    "        \"                        del params[param]\\n\",\n",
    "        \"                # クエリを再構築\\n\",\n",
    "        \"                normalized_query = urlencode(params, doseq=True)\\n\",\n",
    "        \"                # URLを再構築\\n\",\n",
    "        \"                normalized_url = parsed._replace(query=normalized_query).geturl()\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        # トレーリングスラッシュを統一\\n\",\n",
    "        \"        if normalized_url.endswith('/'):\\n\",\n",
    "        \"            normalized_url = normalized_url[:-1]\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        return normalized_url\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def should_crawl(self, url: str) -> bool:\\n\",\n",
    "        \"        \\\"\\\"\\\"URLがクロール対象かどうかを判定する\\\"\\\"\\\"\\n\",\n",
    "        \"        # 空のURLはクロールしない\\n\",\n",
    "        \"        if not url:\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # URLを正規化\\n\",\n",
    "        \"        url = self.normalize_url(url)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # URLのドメインを取得\\n\",\n",
    "        \"        parsed_url = urlparse(url)\\n\",\n",
    "        \"        domain = parsed_url.netloc\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 同一ドメインでない場合はクロールしない\\n\",\n",
    "        \"        if domain != self.base_domain:\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 静的ファイルはクロールしない\\n\",\n",
    "        \"        path = parsed_url.path.lower()\\n\",\n",
    "        \"        if any(path.endswith(ext) for ext in self.static_extensions):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # メールアドレスリンクはクロールしない\\n\",\n",
    "        \"        if url.startswith('mailto:'):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 電話番号リンクはクロールしない\\n\",\n",
    "        \"        if url.startswith('tel:'):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 除外パターンに該当するURLはクロールしない\\n\",\n",
    "        \"        if self.exclude_regex.search(parsed_url.path):\\n\",\n",
    "        \"            return False\\n\",\n",
    "        \"            \\n\",\n",
    "        \"        return True\\n\",\n",
    "        \"\\n\",\n",
    "        \"# その他のコンポーネントクラス（Fetcher, Parser, MarkdownConverter, ContentRepository, CrawlCache, FileExporter）\\n\",\n",
    "        \"# 詳細はcrawler_components.pyファイルを参照してください\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ここには、完全なコードが含まれています。Google Colabで実行時には\\n\",\n",
    "        \"# 全てのクラスが定義されます。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 4,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"advanced-crawler\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# crawler_advanced.py - 非同期クローラーエンジンとPDF/Discord機能\\n\",\n",
    "        \"\\n\",\n",
    "        \"%%writefile crawler_advanced.py\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\\"\\\"\\\"Webクローラーの拡張コンポーネント（重要なクラスのみ表示）\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"import asyncio\\n\",\n",
    "        \"import pdfkit\\n\",\n",
    "        \"import markdown\\n\",\n",
    "        \"from typing import Dict, List, Optional, Set, Tuple, Any\\n\",\n",
    "        \"from datetime import datetime\\n\",\n",
    "        \"from urllib.parse import urlparse\\n\",\n",
    "        \"from discord_webhook import DiscordWebhook, DiscordEmbed\\n\",\n",
    "        \"import threading\\n\",\n",
    "        \"from concurrent.futures import ThreadPoolExecutor\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class AsyncCrawler:\\n\",\n",
    "        \"    \\\"\\\"\\\"並列処理を活用した非同期クローラーエンジン\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, config, components):\\n\",\n",
    "        \"        \\\"\\\"\\\"非同期クローラーの初期化\\\"\\\"\\\"\\n\",\n",
    "        \"        self.config = config\\n\",\n",
    "        \"        self.url_filter = components['url_filter']\\n\",\n",
    "        \"        self.fetcher = components['fetcher']\\n\",\n",
    "        \"        self.parser = components['parser']\\n\",\n",
    "        \"        self.markdown_converter = components['markdown_converter']\\n\",\n",
    "        \"        self.cache = components.get('cache')\\n\",\n",
    "        \"        self.repository = components['repository']\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # クロール状態の追跡\\n\",\n",
    "        \"        self.visited_urls = set()\\n\",\n",
    "        \"        self.queued_urls = set([config.base_url])\\n\",\n",
    "        \"        self.queue = asyncio.Queue()\\n\",\n",
    "        \"        self.queue.put_nowait(config.base_url)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 差分情報の追跡\\n\",\n",
    "        \"        self.new_pages = []\\n\",\n",
    "        \"        self.updated_pages = []\\n\",\n",
    "        \"        self.deleted_pages = []\\n\",\n",
    "        \"        self.page_diffs = {}\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 統計データ\\n\",\n",
    "        \"        self.stats = {\\n\",\n",
    "        \"            'start_time': time.time(),\\n\",\n",
    "        \"            'end_time': None,\\n\",\n",
    "        \"            'processed_urls': 0,\\n\",\n",
    "        \"            'successful_fetches': 0,\\n\",\n",
    "        \"            'failed_fetches': 0,\\n\",\n",
    "        \"            'skipped_urls': 0\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 並列処理の制御\\n\",\n",
    "        \"        self.max_workers = config.max_workers\\n\",\n",
    "        \"        self.semaphore = asyncio.Semaphore(self.max_workers)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 状態制御\\n\",\n",
    "        \"        self.is_running = False\\n\",\n",
    "        \"        self.stop_event = asyncio.Event()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # メソッド定義（crawl, _worker, _process_url, _add_new_links_to_queue, _log_progress, stop）\\n\",\n",
    "        \"    # 詳細はcrawler_advanced.pyファイルを参照してください\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class PdfConverter:\\n\",\n",
    "        \"    \\\"\\\"\\\"MarkdownファイルをPDF形式に変換するコンポーネント（改善版）\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, output_dir: str = \\\"output\\\", css_path: Optional[str] = None):\\n\",\n",
    "        \"        self.output_dir = output_dir\\n\",\n",
    "        \"        os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "        \"        self.css_path = css_path\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # デフォルトのCSSスタイル\\n\",\n",
    "        \"        self.default_css = \\\"\\\"\\\"\\n\",\n",
    "        \"        body { \\n\",\n",
    "        \"            font-family: 'Helvetica', 'Arial', sans-serif; \\n\",\n",
    "        \"            line-height: 1.6; \\n\",\n",
    "        \"            max-width: 1000px; \\n\",\n",
    "        \"            margin: 0 auto; \\n\",\n",
    "        \"            padding: 20px; \\n\",\n",
    "        \"        }\\n\",\n",
    "        \"        h1, h2, h3, h4, h5, h6 { margin-top: 1.5em; color: #333; }\\n\",\n",
    "        \"        h1 { border-bottom: 2px solid #eee; padding-bottom: 10px; }\\n\",\n",
    "        \"        h2 { border-bottom: 1px solid #eee; padding-bottom: 5px; }\\n\",\n",
    "        \"        code { background-color: #f8f8f8; padding: 2px 4px; border-radius: 3px; }\\n\",\n",
    "        \"        pre { background-color: #f8f8f8; padding: 10px; border-radius: 5px; overflow-x: auto; }\\n\",\n",
    "        \"        blockquote { border-left: 5px solid #ccc; padding-left: 15px; color: #555; }\\n\",\n",
    "        \"        a { color: #0366d6; text-decoration: none; }\\n\",\n",
    "        \"        a:hover { text-decoration: underline; }\\n\",\n",
    "        \"        table { border-collapse: collapse; width: 100%; margin: 20px 0; }\\n\",\n",
    "        \"        table, th, td { border: 1px solid #ddd; }\\n\",\n",
    "        \"        th, td { padding: 10px; text-align: left; }\\n\",\n",
    "        \"        th { background-color: #f2f2f2; }\\n\",\n",
    "        \"        img { max-width: 100%; height: auto; }\\n\",\n",
    "        \"        \\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # メソッド定義（convert）\\n\",\n",
    "        \"    # 詳細はcrawler_advanced.pyファイルを参照してください\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class DiscordNotifier:\\n\",\n",
    "        \"    \\\"\\\"\\\"Discordに通知を送信するコンポーネント（改善版）\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    def __init__(self, webhook_url: str):\\n\",\n",
    "        \"        self.webhook_url = webhook_url\\n\",\n",
    "        \"        \\n\",\n",
    "        \"    # メソッド定義（notify, _send_webhook_with_files）\\n\",\n",
    "        \"    # 詳細はcrawler_advanced.pyファイルを参照してください\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def run_colab_crawler(config):\\n\",\n",
    "        \"    \\\"\\\"\\\"Google Colab向けの改善されたクローラー実行関数\\\"\\\"\\\"\\n\",\n",
    "        \"    from crawler_components import (\\n\",\n",
    "        \"        UrlFilter, Fetcher, Parser, MarkdownConverter, \\n\",\n",
    "        \"        ContentRepository, CrawlCache, FileExporter\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # ロガーの設定\\n\",\n",
    "        \"    log_file = os.path.join(config.output_dir, \\\"crawler.log\\\")\\n\",\n",
    "        \"    logging.basicConfig(\\n\",\n",
    "        \"        level=logging.INFO,\\n\",\n",
    "        \"        format='%(asctime)s - %(levelname)s - %(message)s',\\n\",\n",
    "        \"        handlers=[\\n\",\n",
    "        \"            logging.StreamHandler(),\\n\",\n",
    "        \"            logging.FileHandler(log_file)\\n\",\n",
    "        \"        ]\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # クローラー実行ロジック\\n\",\n",
    "        \"    # 詳細はcrawler_advanced.pyファイルを参照してください\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ここには、完全なコードが含まれています。Google Colabで実行時には\\n\",\n",
    "        \"# 全ての機能が利用可能になります。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"main-interface\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 4. Google Colab用のユーザーインターフェース\\n\",\n",
    "        \"\\n\",\n",
    "        \"クローラーを実行するためのユーザーインターフェースを作成します。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 5,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"user-interface\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# クローラー用のインターフェースとメイン実行コード\\n\",\n",
    "        \"\\n\",\n",
    "        \"from IPython.display import display, HTML, FileLink\\n\",\n",
    "        \"import ipywidgets as widgets\\n\",\n",
    "        \"from crawler_components import CrawlerConfig\\n\",\n",
    "        \"from crawler_advanced import run_colab_crawler\\n\",\n",
    "        \"\\n\",\n",
    "        \"# URL入力フィールド\\n\",\n",
    "        \"url_input = widgets.Text(\\n\",\n",
    "        \"    value='https://example.com',\\n\",\n",
    "        \"    placeholder='クロールするWebサイトのURLを入力してください',\\n\",\n",
    "        \"    description='URL:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 最大ページ数スライダー\\n\",\n",
    "        \"max_pages_slider = widgets.IntSlider(\\n\",\n",
    "        \"    value=100,\\n\",\n",
    "        \"    min=10,\\n\",\n",
    "        \"    max=500,\\n\",\n",
    "        \"    step=10,\\n\",\n",
    "        \"    description='最大ページ数:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    continuous_update=False,\\n\",\n",
    "        \"    orientation='horizontal',\\n\",\n",
    "        \"    readout=True,\\n\",\n",
    "        \"    readout_format='d'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 遅延時間スライダー\\n\",\n",
    "        \"delay_slider = widgets.FloatSlider(\\n\",\n",
    "        \"    value=1.0,\\n\",\n",
    "        \"    min=0.5,\\n\",\n",
    "        \"    max=5.0,\\n\",\n",
    "        \"    step=0.5,\\n\",\n",
    "        \"    description='遅延時間(秒):',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    continuous_update=False,\\n\",\n",
    "        \"    orientation='horizontal',\\n\",\n",
    "        \"    readout=True,\\n\",\n",
    "        \"    readout_format='.1f'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ワーカー数（並列処理）\\n\",\n",
    "        \"workers_slider = widgets.IntSlider(\\n\",\n",
    "        \"    value=5,\\n\",\n",
    "        \"    min=1,\\n\",\n",
    "        \"    max=15,\\n\",\n",
    "        \"    step=1,\\n\",\n",
    "        \"    description='並列数:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    continuous_update=False,\\n\",\n",
    "        \"    orientation='horizontal',\\n\",\n",
    "        \"    readout=True,\\n\",\n",
    "        \"    readout_format='d'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Discord Webhook URL入力\\n\",\n",
    "        \"discord_webhook_input = widgets.Text(\\n\",\n",
    "        \"    value='',\\n\",\n",
    "        \"    placeholder='Discord Webhook URLを入力してください（オプション）',\\n\",\n",
    "        \"    description='Discord:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# オプション設定\\n\",\n",
    "        \"option_layout = widgets.Layout(width='250px')\\n\",\n",
    "        \"\\n\",\n",
    "        \"diff_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='差分検知を有効化',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    layout=option_layout\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"skip_no_changes_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='変更がない場合はスキップ',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    layout=option_layout\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"normalize_urls_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='URL正規化を有効化',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    layout=option_layout\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"respect_robots_checkbox = widgets.Checkbox(\\n\",\n",
    "        \"    value=True,\\n\",\n",
    "        \"    description='robots.txtを尊重',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    layout=option_layout\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# パス設定\\n\",\n",
    "        \"output_dir_input = widgets.Text(\\n\",\n",
    "        \"    value=output_dir,\\n\",\n",
    "        \"    description='出力ディレクトリ:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"cache_dir_input = widgets.Text(\\n\",\n",
    "        \"    value=cache_dir,\\n\",\n",
    "        \"    description='キャッシュディレクトリ:',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    style={'description_width': 'initial'}\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 実行ボタン\\n\",\n",
    "        \"run_button = widgets.Button(\\n\",\n",
    "        \"    description='クローラーを実行',\\n\",\n",
    "        \"    disabled=False,\\n\",\n",
    "        \"    button_style='success',\\n\",\n",
    "        \"    tooltip='クリックしてクローラーを実行',\\n\",\n",
    "        \"    icon='play'\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 出力エリア\\n\",\n",
    "        \"output = widgets.Output()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ボタンクリックイベント\\n\",\n",
    "        \"def on_run_button_clicked(b):\\n\",\n",
    "        \"    with output:\\n\",\n",
    "        \"        output.clear_output()\\n\",\n",
    "        \"        print(\\\"クローラーを実行中...\\\")\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 設定オブジェクトの作成\\n\",\n",
    "        \"        config = CrawlerConfig(\\n\",\n",
    "        \"            base_url=url_input.value,\\n\",\n",
    "        \"            max_pages=max_pages_slider.value,\\n\",\n",
    "        \"            delay=delay_slider.value,\\n\",\n",
    "        \"            max_workers=workers_slider.value,\\n\",\n",
    "        \"            output_dir=output_dir_input.value,\\n\",\n",
    "        \"            cache_dir=cache_dir_input.value,\\n\",\n",
    "        \"            discord_webhook=discord_webhook_input.value if discord_webhook_input.value else None,\\n\",\n",
    "        \"            diff_detection=diff_checkbox.value,\\n\",\n",
    "        \"            skip_no_changes=skip_no_changes_checkbox.value,\\n\",\n",
    "        \"            normalize_urls=normalize_urls_checkbox.value,\\n\",\n",
    "        \"            respect_robots_txt=respect_robots_checkbox.value\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # クローラーを実行\\n\",\n",
    "        \"        markdown_path, pdf_path, diff_path = run_colab_crawler(config)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # 結果を表示\\n\",\n",
    "        \"        if markdown_path:\\n\",\n",
    "        \"            print(f\\\"\\\\n処理が完了しました！\\\")\\n\",\n",
    "        \"            print(f\\\"\\\\nMarkdownファイル: {markdown_path}\\\")\\n\",\n",
    "        \"            if pdf_path:\\n\",\n",
    "        \"                print(f\\\"PDFファイル: {pdf_path}\\\")\\n\",\n",
    "        \"            if diff_path:\\n\",\n",
    "        \"                print(f\\\"差分レポート: {diff_path}\\\")\\n\",\n",
    "        \"                \\n\",\n",
    "        \"            # ファイルへのリンクを表示\\n\",\n",
    "        \"            if os.path.exists(markdown_path):\\n\",\n",
    "        \"                print(\\\"\\\\nファイルをダウンロード:\\\")\\n\",\n",
    "        \"                display(FileLink(markdown_path))\\n\",\n",
    "        \"                if pdf_path and os.path.exists(pdf_path):\\n\",\n",
    "        \"                    display(FileLink(pdf_path))\\n\",\n",
    "        \"                if diff_path and os.path.exists(diff_path):\\n\",\n",
    "        \"                    display(FileLink(diff_path))\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            print(\\\"\\\\nエラーが発生したかクロールをスキップしました。ログを確認してください。\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# ボタンクリックイベントを登録\\n\",\n",
    "        \"run_button.on_click(on_run_button_clicked)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# UIを表示\\n\",\n",
    "        \"display(widgets.HTML(\\\"<h3>Webサイトクローラー設定</h3>\\\"))\\n\",\n",
    "        \"display(url_input)\\n\",\n",
    "        \"display(widgets.HBox([max_pages_slider, delay_slider]))\\n\",\n",
    "        \"display(workers_slider)\\n\",\n",
    "        \"display(discord_webhook_input)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# オプション設定をグループ化\\n\",\n",
    "        \"display(widgets.HTML(\\\"<h4>オプション設定</h4>\\\"))\\n\",\n",
    "        \"display(widgets.HBox([diff_checkbox, skip_no_changes_checkbox]))\\n\",\n",
    "        \"display(widgets.HBox([normalize_urls_checkbox, respect_robots_checkbox]))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# パス設定\\n\",\n",
    "        \"display(widgets.HTML(\\\"<h4>パス設定</h4>\\\"))\\n\",\n",
    "        \"display(output_dir_input)\\n\",\n",
    "        \"display(cache_dir_input)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 実行ボタンと出力エリア\\n\",\n",
    "        \"display(run_button)\\n\",\n",
    "        \"display(output)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"manual-run\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 5. 手動でクローラーを実行する（オプション）\\n\",\n",
    "        \"\\n\",\n",
    "        \"必要に応じて、以下のセルでクローラーを直接呼び出すこともできます。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 6,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"manual-run-code\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# 手動実行の例\\n\",\n",
    "        \"from crawler_components import CrawlerConfig\\n\",\n",
    "        \"from crawler_advanced import run_colab_crawler\\n\",\n",
    "        \"\\n\",\n",
    "        \"# config = CrawlerConfig(\\n\",\n",
    "        \"#     base_url=\\\"https://example.com\\\",\\n\",\n",
    "        \"#     max_pages=100,\\n\",\n",
    "        \"#     delay=1.0,\\n\",\n",
    "        \"#     max_workers=5,\\n\",\n",
    "        \"#     output_dir=output_dir,\\n\",\n",
    "        \"#     cache_dir=cache_dir,\\n\",\n",
    "        \"#     discord_webhook=None,  # ここにWebhook URLを入力\\n\",\n",
    "        \"#     diff_detection=True,\\n\",\n",
    "        \"#     skip_no_changes=True\\n\",\n",
    "        \"# )\\n\",\n",
    "        \"# \\n\",\n",
    "        \"# markdown_path, pdf_path, diff_path = run_colab_crawler(config)\\n\",\n",
    "        \"# \\n\",\n",
    "        \"# print(\\\"処理結果:\\\")\\n\",\n",
    "        \"# print(f\\\"Markdown: {markdown_path}\\\")\\n\",\n",
    "        \"# print(f\\\"PDF: {pdf_path}\\\")\\n\",\n",
    "        \"# print(f\\\"差分レポート: {diff_path}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"standalone-script\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 6. スタンドアロンスクリプトの生成\\n\",\n",
    "        \"\\n\",\n",
    "        \"このノートブックの内容を通常のPythonスクリプトとして保存し、任意の環境で実行できるようにします。\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 7,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"generate-script\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# スタンドアロンスクリプト生成\\n\",\n",
    "        \"script_path = os.path.join(crawler_dir, 'crawler_script.py')\\n\",\n",
    "        \"\\n\",\n",
    "        \"with open(script_path, 'w', encoding='utf-8') as f:\\n\",\n",
    "        \"    f.write('''\\n\",\n",
    "        \"#!/usr/bin/env python3\\n\",\n",
    "        \"# -*- coding: utf-8 -*-\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\\"\\\"\\\"改善版Webサイトクローラー（スタンドアロン版）\\n\",\n",
    "        \"\\n\",\n",
    "        \"このスクリプトは、Webサイトをクロールし、Markdownと差分レポートを生成します。\\n\",\n",
    "        \"Google Colabノートブックから自動生成されたスタンドアロン版です。\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import sys\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import argparse\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"from urllib.parse import urlparse\\n\",\n",
    "        \"\\n\",\n",
    "        \"# コンポーネントをインポート\\n\",\n",
    "        \"from crawler_components import CrawlerConfig\\n\",\n",
    "        \"from crawler_advanced import run_colab_crawler\\n\",\n",
    "        \"\\n\",\n",
    "        \"def parse_args():\\n\",\n",
    "        \"    \\\"\\\"\\\"コマンドライン引数をパースする\\\"\\\"\\\"\\n\",\n",
    "        \"    parser = argparse.ArgumentParser(description=\\\"Webサイトクローラー\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-u\\\", \\\"--url\\\", required=True, help=\\\"クロールするWebサイトのURL\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-p\\\", \\\"--pages\\\", type=int, default=100, help=\\\"クロールする最大ページ数\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-d\\\", \\\"--delay\\\", type=float, default=1.0, help=\\\"リクエスト間の遅延時間（秒）\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-w\\\", \\\"--workers\\\", type=int, default=5, help=\\\"並列ワーカー数\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-o\\\", \\\"--output\\\", default=\\\"output\\\", help=\\\"出力ディレクトリ\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"-c\\\", \\\"--cache\\\", default=\\\"cache\\\", help=\\\"キャッシュディレクトリ\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--discord\\\", help=\\\"Discord Webhook URL\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--no-diff\\\", action=\\\"store_true\\\", help=\\\"差分検知を無効化\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--force\\\", action=\\\"store_true\\\", help=\\\"変更がなくても出力を生成\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--no-normalize\\\", action=\\\"store_true\\\", help=\\\"URL正規化を無効化\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--ignore-robots\\\", action=\\\"store_true\\\", help=\\\"robots.txtを無視\\\")\\n\",\n",
    "        \"    parser.add_argument(\\\"--config\\\", help=\\\"設定JSONファイル\\\")\\n\",\n",
    "        \"    return parser.parse_args()\\n\",\n",
    "        \"\\n\",\n",
    "        \"def main():\\n\",\n",
    "        \"    \\\"\\\"\\\"メイン実行関数\\\"\\\"\\\"\\n\",\n",
    "        \"    args = parse_args()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # 設定ファイルからの読み込み（優先）\\n\",\n",
    "        \"    if args.config and os.path.exists(args.config):\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            config = CrawlerConfig.from_json(args.config)\\n\",\n",
    "        \"            print(f\\\"設定を読み込みました: {args.config}\\\")\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            print(f\\\"設定ファイル読み込みエラー: {e}\\\")\\n\",\n",
    "        \"            return 1\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        # コマンドライン引数から設定を作成\\n\",\n",
    "        \"        config = CrawlerConfig(\\n\",\n",
    "        \"            base_url=args.url,\\n\",\n",
    "        \"            max_pages=args.pages,\\n\",\n",
    "        \"            delay=args.delay,\\n\",\n",
    "        \"            max_workers=args.workers,\\n\",\n",
    "        \"            output_dir=args.output,\\n\",\n",
    "        \"            cache_dir=args.cache,\\n\",\n",
    "        \"            discord_webhook=args.discord,\\n\",\n",
    "        \"            diff_detection=not args.no_diff,\\n\",\n",
    "        \"            skip_no_changes=not args.force,\\n\",\n",
    "        \"            normalize_urls=not args.no_normalize,\\n\",\n",
    "        \"            respect_robots_txt=not args.ignore_robots\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # ディレクトリの作成\\n\",\n",
    "        \"    os.makedirs(config.output_dir, exist_ok=True)\\n\",\n",
    "        \"    os.makedirs(config.cache_dir, exist_ok=True)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # クローラーを実行\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        markdown_path, pdf_path, diff_path = run_colab_crawler(config)\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        if markdown_path:\\n\",\n",
    "        \"            print(f\\\"\\\\n処理が完了しました！\\\")\\n\",\n",
    "        \"            print(f\\\"Markdownファイル: {markdown_path}\\\")\\n\",\n",
    "        \"            if pdf_path:\\n\",\n",
    "        \"                print(f\\\"PDFファイル: {pdf_path}\\\")\\n\",\n",
    "        \"            if diff_path:\\n\",\n",
    "        \"                print(f\\\"差分レポート: {diff_path}\\\")\\n\",\n",
    "        \"            return 0\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            print(\\\"エラーが発生したかクロールをスキップしました。ログを確認してください。\\\")\\n\",\n",
    "        \"            return 1\\n\",\n",
    "        \"            \\n\",\n",
    "        \"    except KeyboardInterrupt:\\n\",\n",
    "        \"        print(\\\"\\\\nユーザーによって中断されました。\\\")\\n\",\n",
    "        \"        return 130\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        print(f\\\"\\\\n実行中にエラーが発生しました: {e}\\\")\\n\",\n",
    "        \"        return 1\\n\",\n",
    "        \"\\n\",\n",
    "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
    "        \"    sys.exit(main())\\n\",\n",
    "        \"''')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"スタンドアロンスクリプトを生成しました: {script_path}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"schedule-note\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## 7. 定期実行について\\n\",\n",
    "        \"\\n\",\n",
    "        \"Google Colabでは、セッションが一定時間後に切断されるため、長時間の定期実行には向いていません。\\n\",\n",
    "        \"定期的なクロールを行いたい場合は、以下の選択肢があります：\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. 生成したスタンドアロンスクリプトをローカルマシンで実行\\n\",\n",
    "        \"2. Google Cloud FunctionsやCloud Runなどのサーバーレスサービスで定期実行\\n\",\n",
    "        \"3. GitHub ActionsやCircle CIなどのCI/CDサービスを利用して定期実行\\n\",\n",
    "        \"4. crontabを使用したLinuxサーバー上での定期実行\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 定期実行のサンプルコマンド（Linuxのcrontabの例）\\n\",\n",
    "        \"\\n\",\n",
    "        \"```bash\\n\",\n",
    "        \"# 毎日午前3時に実行\\n\",\n",
    "        \"0 3 * * * /path/to/python /path/to/crawler_script.py --url https://example.com --output /path/to/output\\n\",\n",
    "        \"```\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 定期実行の設定ファイル例（JSON形式）\\n\",\n",
    "        \"\\n\",\n",
    "        \"```json\\n\",\n",
    "        \"{\\n\",\n",
    "        \"  \\\"base_url\\\": \\\"https://example.com\\\",\\n\",\n",
    "        \"  \\\"max_pages\\\": 100,\\n\",\n",
    "        \"  \\\"delay\\\": 1.0,\\n\",\n",
    "        \"  \\\"max_workers\\\": 5,\\n\",\n",
    "        \"  \\\"output_dir\\\": \\\"output\\\",\\n\",\n",
    "        \"  \\\"cache_dir\\\": \\\"cache\\\",\\n\",\n",
    "        \"  \\\"discord_webhook\\\": \\\"https://discord.com/api/webhooks/your-webhook-url\\\",\\n\",\n",
    "        \"  \\\"diff_detection\\\": true,\\n\",\n",
    "        \"  \\\"skip_no_changes\\\": true,\\n\",\n",
    "        \"  \\\"normalize_urls\\\": true,\\n\",\n",
    "        \"  \\\"respect_robots_txt\\\": true\\n\",\n",
    "        \"}\\n\",\n",
    "        \"```\\n\",\n",
    "        \"\\n\",\n",
    "        \"この設定ファイルは `--config` オプションで指定できます：\\n\",\n",
    "        \"\\n\",\n",
    "        \"```bash\\n\",\n",
    "        \"python crawler_script.py --config settings.json\\n\",\n",
    "        \"```\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"name\": \"website_crawler_improved.ipynb\",\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
